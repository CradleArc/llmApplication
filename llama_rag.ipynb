{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T16:59:58.694121500Z",
     "start_time": "2024-05-29T16:59:10.923547600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting langchain\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ff/39/62b8d2eb468a3b924c6bd1e411ef969c340130f4833abee35c15f0df421b/langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "Collecting weaviate-client\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5f/54/f450f3b25d847c4f9d6cbb89b2627e7febe05f4d09e54de58db9334af6b3/weaviate_client-4.6.3-py3-none-any.whl (324 kB)\n",
      "     -------------------------------------- 324.9/324.9 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/74/9a/eec023807ae78e83342567303916b34a348d9d40703e7cef5dfb1e3635b6/SQLAlchemy-2.0.30-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/a4/69/0d415c6d8450842652ce01b29f43416a0f30122b75899de01485623c7850/aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c1/d9/42bb3c89db1b21d3de5fdebfbdd695c5e0722273935c9ac4a2d84f4ec381/langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/21/3f/84a4adc8838c3fda0fa38f71ec751365b8c8d058b8be7fbd779cb6e63e1b/langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/bf/47/5e218733516797e7dcd6f2f67efd458de99ce5181945753f3b9cd62bd40c/langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
      "Collecting numpy<2,>=1\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/3f/6b/5610004206cf7f8e7ad91c5a85a8c71b2f2f8051a0c0c4d5916b76d6cbb2/numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Collecting pydantic<3,>=1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6f/b9/ec44b1394957d5aa8d3a7c33f8304cd7670d10a43a286db56cec086346be/pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "     -------------------------------------- 409.5/409.5 kB 6.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/61/a1/6bb0cbebefb23641f068bb58a2bc56da9beb2b1c550242e3c540b37698f3/tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: httpx<=0.27.0,>=0.25.0 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from weaviate-client) (0.27.0)\n",
      "Collecting validators==0.28.1\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/e8/a4/40155f725ccb1b4e0f237b435a7664ea54146efb15ef70948e494b6acac4/validators-0.28.1-py3-none-any.whl (39 kB)\n",
      "Collecting authlib<2.0.0,>=1.2.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/25/65/b78eb948b71ab232d08b30c38a2e3b69e6e50c6e166863a0068c877155b9/Authlib-1.3.0-py2.py3-none-any.whl (223 kB)\n",
      "     -------------------------------------- 223.7/223.7 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0.0,>=1.57.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bc/4f/22cdc1d2073593aac4c0cfee7d69418c6dad24069372e3f6e6706673daa6/grpcio-1.64.0-cp311-cp311-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 5.6 MB/s eta 0:00:00\n",
      "Collecting grpcio-tools<2.0.0,>=1.57.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/76/b3/524a445977d08a39358180b92ab97bb9a12ecc1ef763a5a3f62a50fa0b9c/grpcio_tools-1.64.0-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 6.2 MB/s eta 0:00:00\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.57.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/39/25/40119604fc7dd2483b376316db02cb2dd58b976af7ae887716a2d25fbc19/grpcio_health_checking-1.64.0-py3-none-any.whl (18 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/b3/21/c5aaffac47fd305d69df46cfbf118768cdf049a92ee6b0b5cb029d449dcf/frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/88/aa/ea217cb18325aa05cb3e3111c19715f1e97c50a4a900cbc20e54648de5f5/multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/27/41/945ae9a80590e4fb0be166863c6e63d75e4b35789fa3a61ff1dbdcdc220f/yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Collecting cryptography\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/0d/59/17c9070c5c397881900c868a5c4e4a522437e3e1653294925082e5f6cf0b/cryptography-42.0.7-cp39-abi3-win_amd64.whl (2.9 MB)\n",
      "Collecting protobuf<6.0dev,>=5.26.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7f/75/8d4c58ec98e50d229921db75d4fbdf3dcb2a2b3889004a03885b91322041/protobuf-5.27.0-cp310-abi3-win_amd64.whl (426 kB)\n",
      "     -------------------------------------- 426.9/426.9 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (65.5.1)\n",
      "Requirement already satisfied: anyio in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (4.4.0)\n",
      "Requirement already satisfied: certifi in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.0.5)\n",
      "Requirement already satisfied: idna in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.7)\n",
      "Requirement already satisfied: sniffio in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from httpcore==1.*->httpx<=0.27.0,>=0.25.0->weaviate-client) (0.14.0)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/f9/b7/3815984df03b677644c90cd4893d6293c80ef1c9f3a8493807bc1eb47da7/orjson-3.10.3-cp311-none-win_amd64.whl (138 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.18.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d2/c7/e01cb2017c4b7b274258694f73e8bbbb0988a28b49802e569d1d9bfd51cb/pydantic_core-2.18.3-cp311-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/47/79/26d54d7d700ef65b689fc2665a40846d13e834da0486674a8d4f0f371a47/greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
      "Requirement already satisfied: pycparser in d:\\system_default\\desktop\\github_repo_code\\llmapplication\\venv\\lib\\site-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n",
      "Installing collected packages: validators, tenacity, pydantic-core, protobuf, packaging, orjson, numpy, multidict, jsonpatch, grpcio, greenlet, frozenlist, annotated-types, yarl, SQLAlchemy, pydantic, grpcio-tools, grpcio-health-checking, cryptography, aiosignal, langsmith, authlib, aiohttp, weaviate-client, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 authlib-1.3.0 cryptography-42.0.7 frozenlist-1.4.1 greenlet-3.0.3 grpcio-1.64.0 grpcio-health-checking-1.64.0 grpcio-tools-1.64.0 jsonpatch-1.33 langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langsmith-0.1.63 multidict-6.0.5 numpy-1.26.4 orjson-3.10.3 packaging-23.2 protobuf-5.27.0 pydantic-2.7.2 pydantic-core-2.18.3 tenacity-8.3.0 validators-0.28.1 weaviate-client-4.6.3 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RAG需要从向量数据库检索上下文然后输入LLM进行生成，因此需要提前将文本数据向量化并存储到向量数据库。主要步骤如下：\n",
    "准备文本资料\n",
    "将文本分块\n",
    "嵌入以及存储块到向量数据库"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocument_loaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TextLoader \u001B[38;5;66;03m# 文本加载器\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharacterTextSplitter \u001B[38;5;66;03m# 文本分块器\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OllamaEmbeddings \u001B[38;5;66;03m# Ollama向量嵌入器\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader # 文本加载器\n",
    "from langchain.text_splitter import CharacterTextSplitter # 文本分块器\n",
    "from langchain_community.embeddings import OllamaEmbeddings # Ollama向量嵌入器\n",
    "import weaviate # 向量数据库\n",
    "from weaviate.embedded import EmbeddedOptions # 向量嵌入选项\n",
    "from langchain.prompts import ChatPromptTemplate # 聊天提示模板\n",
    "from langchain_community.chat_models import ChatOllama # ChatOllma聊天模型\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser # 输出解析器\n",
    "from langchain_community.vectorstores import Weaviate # 向量数据库\n",
    "import requests"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:00:24.470426800Z",
     "start_time": "2024-05-29T17:00:23.201843200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 下载&加载语料\n",
    "这里使用拜登总统2022年的国情咨文作为示例。文件链接https://raw.githubusercontent.com/langchain-ai/langchain/mast...。langchain提供了多个文档加载器，这里我们使用TextLoaders即可。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 下载文件\n",
    "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "# 加载文件\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于原始文档过大，超出了LLM的上下文窗口，需要将其分块才能让LLM识别。LangChain 提供了许多内置的文本分块工具，这里用CharacterTextSplitter作为示例："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "嵌入以及存储到向量数据库\n",
    "为了对语料分块进行搜索，需要为每个块生成向量并嵌入文档，最后将文档和向量一起存储。这里使用Ollama&llama3生成向量，并存储到Weaviate向量数据库。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions()\n",
    ")\n",
    "print(\"store vector\")\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client=client,\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3\"),\n",
    "    by_text=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "检索 & 增强\n",
    "向量数据库加载数据后，可以作为检索器，通过用户查询和嵌入向量之间的语义相似性获取数据，然后使用一个固定的聊天模板即可。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 检索器\n",
    "retriever = vectorstore.as_retriever()\n",
    "# LLM提示模板\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "   Use the following pieces of retrieved context to answer the question.\n",
    "   If you don't know the answer, just say that you don't know.\n",
    "   Use three sentences maximum and keep the answer concise.\n",
    "   Question: {question}\n",
    "   Context: {context}\n",
    "   Answer:\n",
    "   \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "生成\n",
    "最后，将检索器、聊天模板以及LLM组合成RAG链就可以了。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3\", temperature=10)\n",
    "rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} # 上下文信息\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "# 开始查询&生成\n",
    "query = \"What did the president mainly say?\"\n",
    "print(rag_chain.invoke(query))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到还是像那么回事的，LLM使用的输入预料的内容答复了一些关于新冠疫情以及工作、社区等内容。\n",
    "\n",
    "langchain支持多种LLM，有需要的读者可以尝试下使用OpenAI提供的LLM。\n",
    "读者可以根据需要替换下输入预料，构造自己的私有知识检索库。\n",
    "\n",
    "本文所有代码如下："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import requests\n",
    "# 下载数据\n",
    "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "# 加载数据\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "# 文本分块\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "# 初始化向量数据库并嵌入目标文档\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions()\n",
    ")\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client=client,\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3\"),\n",
    "    by_text=False\n",
    ")\n",
    "# 检索器\n",
    "retriever = vectorstore.as_retriever()\n",
    "# LLM提示模板\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "   Use the following pieces of retrieved context to answer the question.\n",
    "   If you don't know the answer, just say that you don't know.\n",
    "   Use three sentences maximum and keep the answer concise.\n",
    "   Question: {question}\n",
    "   Context: {context}\n",
    "   Answer:\n",
    "   \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOllama(model=\"llama3\", temperature=10)\n",
    "rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "# 开始查询&生成\n",
    "query = \"What did the president mainly say?\"\n",
    "print(rag_chain.invoke(query))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
