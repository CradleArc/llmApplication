{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"Hello\": \"World\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "# use bf16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "# use fp16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# use auto mode, automatically select precision based on the device.\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n",
    "# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "\n",
    "# 第一轮对话 1st dialogue turn\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)\n",
    "# 你好！很高兴为你提供帮助。\n",
    "\n",
    "# 第二轮对话 2nd dialogue turn\n",
    "response, history = model.chat(tokenizer, \"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\n",
    "print(response)\n",
    "# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。\n",
    "# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。\n",
    "# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。\n",
    "# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。\n",
    "# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。\n",
    "# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。\n",
    "\n",
    "# 第三轮对话 3rd dialogue turn\n",
    "response, history = model.chat(tokenizer, \"给这个故事起一个标题\", history=history)\n",
    "print(response)\n",
    "# 《奋斗创业：一个年轻人的成功之路》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import uvicorn\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "# 设置设备参数\n",
    "DEVICE = \"cuda\"  # 使用CUDA\n",
    "DEVICE_ID = \"0\"  # CUDA设备ID，如果未设置则为空\n",
    "CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE  # 组合CUDA设备信息\n",
    "\n",
    "# 清理GPU内存函数\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():  # 检查是否可用CUDA\n",
    "        with torch.cuda.device(CUDA_DEVICE):  # 指定CUDA设备\n",
    "            torch.cuda.empty_cache()  # 清空CUDA缓存\n",
    "            torch.cuda.ipc_collect()  # 收集CUDA内存碎片\n",
    "\n",
    "# 创建FastAPI应用\n",
    "app = FastAPI()\n",
    "\n",
    "# 处理POST请求的端点\n",
    "@app.post(\"/\")\n",
    "async def create_item(request: Request):\n",
    "    global model, tokenizer  # 声明全局变量以便在函数内部使用模型和分词器\n",
    "    json_post_raw = await request.json()  # 获取POST请求的JSON数据\n",
    "    json_post = json.dumps(json_post_raw)  # 将JSON数据转换为字符串\n",
    "    json_post_list = json.loads(json_post)  # 将字符串转换为Python对象\n",
    "    prompt = json_post_list.get('prompt')  # 获取请求中的提示\n",
    "    history = json_post_list.get('history')  # 获取请求中的历史记录\n",
    "    max_length = json_post_list.get('max_length')  # 获取请求中的最大长度\n",
    "    top_p = json_post_list.get('top_p')  # 获取请求中的top_p参数\n",
    "    temperature = json_post_list.get('temperature')  # 获取请求中的温度参数\n",
    "    # 调用模型进行对话生成\n",
    "    response, history = model.chat(\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        history=history,\n",
    "        max_length=max_length if max_length else 2048,  # 如果未提供最大长度，默认使用2048\n",
    "        top_p=top_p if top_p else 0.7,  # 如果未提供top_p参数，默认使用0.7\n",
    "        temperature=temperature if temperature else 0.95  # 如果未提供温度参数，默认使用0.95\n",
    "    )\n",
    "    now = datetime.datetime.now()  # 获取当前时间\n",
    "    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")  # 格式化时间为字符串\n",
    "    # 构建响应JSON\n",
    "    answer = {\n",
    "        \"response\": response,\n",
    "        \"history\": history,\n",
    "        \"status\": 200,\n",
    "        \"time\": time\n",
    "    }\n",
    "    # 构建日志信息\n",
    "    log = \"[\" + time + \"] \" + '\", prompt:\"' + prompt + '\", response:\"' + repr(response) + '\"'\n",
    "    print(log)  # 打印日志\n",
    "    torch_gc()  # 执行GPU内存清理\n",
    "    return answer  # 返回响应\n",
    "\n",
    "# 主函数入口\n",
    "if __name__ == '__main__':\n",
    "    # 加载预训练的分词器和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True).eval()\n",
    "    model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat\", trust_remote_code=True) # 可指定\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    # 启动FastAPI应用，用6006端口映射到本地，从而在本地使用api\n",
    "    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)  # 在指定端口和主机上启动应用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
