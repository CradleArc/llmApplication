{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eeb7eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id author     source content  \\\n",
      "100   89517    NaN  中国证券报?中证网     NaN   \n",
      "103   89514    NaN  中国证券报?中证网     NaN   \n",
      "997   88620    NaN        央广网     NaN   \n",
      "1273  88344    NaN        央广网     NaN   \n",
      "1282  88335    NaN        央广网     NaN   \n",
      "\n",
      "                                                feature  \\\n",
      "100   {\"type\":\"公司\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
      "103   {\"type\":\"公司\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
      "997   {\"type\":\"时事要闻\",\"site\":\"参考消息\",\"commentNum\":\"0\",...   \n",
      "1273  {\"type\":\"IT业界\",\"site\":\"参考消息\",\"commentNum\":\"0\",...   \n",
      "1282  {\"type\":\"IT业界\",\"site\":\"参考消息\",\"commentNum\":\"0\",...   \n",
      "\n",
      "                                 title  \\\n",
      "100       天和防务股东未来6个月内计划减持不超过480万股公司股份   \n",
      "103                    晶盛机电调整限制性股票回购价格   \n",
      "997              [主播不在家]第二季：主播陈亮体验垃圾清运   \n",
      "1273                LKK洛可可：想象力经济时代或已到来   \n",
      "1282  CES2017：京东发布两款叮咚智能音箱新品 开放Alpha平台   \n",
      "\n",
      "                                                    url  \n",
      "100   http://www.cs.com.cn/ssgs/gsxw/201706/t2017062...  \n",
      "103   http://www.cs.com.cn/ssgs/gsxw/201706/t2017062...  \n",
      "997   http://www.cankaoxiaoxi.com/china/20170623/214...  \n",
      "1273  http://www.cankaoxiaoxi.com/science/20170610/2...  \n",
      "1282  http://www.cankaoxiaoxi.com/science/20170610/2...  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chinese_stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m news\u001b[38;5;241m=\u001b[39mnews\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 加载停用词\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchinese_stopwords.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     19\u001b[0m     stopwords\u001b[38;5;241m=\u001b[39m[i[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 分词\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chinese_stopwords.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "path = './text_similarity/'\n",
    "# 数据加载\n",
    "news = pd.read_csv(path+'sqlResult.csv',encoding='gb18030')\n",
    "# 处理缺失值\n",
    "print(news[news.content.isna()].head(5))\n",
    "news=news.dropna(subset=['content'])\n",
    "\n",
    "# 加载停用词\n",
    "with open(path+'chinese_stopwords.txt','r', encoding='utf-8') as file:\n",
    "    stopwords=[i[:-1] for i in file.readlines()]\n",
    "    \n",
    "# 分词\n",
    "def split_text(text):\n",
    "    #return ' '.join([w for w in list(jieba.cut(re.sub('\\s|[%s]' % (punctuation),'',text))) if w not in stopwords])\n",
    "    text = text.replace(' ', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text2 = jieba.cut(text.strip())\n",
    "    result = ' '.join([w for w in text2 if w not in stopwords])\n",
    "    return result\n",
    "\n",
    "# 加载清洗后的corpus\n",
    "if not os.path.exists(path+\"corpus.pkl\"):\n",
    "    # 对所有文本进行分词\n",
    "    corpus=list(map(split_text,[str(i) for i in news.content]))\n",
    "    print(corpus[0])\n",
    "    print(len(corpus))\n",
    "    print(corpus[1])\n",
    "    # 保存到文件，方便下次调用\n",
    "    with open(path+'corpus.pkl','wb') as file:\n",
    "        pickle.dump(corpus, file)\n",
    "else:\n",
    "    # 调用上次处理的结果\n",
    "    with open(path+'corpus.pkl','rb') as file:\n",
    "        corpus = pickle.load(file)\n",
    "\n",
    "# 得到corpus的TF-IDF矩阵\n",
    "if not os.path.exists(path+\"tfidf.pkl\"):\n",
    "    countvectorizer = CountVectorizer(encoding='gb18030',min_df=0.015)\n",
    "    tfidftransformer = TfidfTransformer()\n",
    "    countvector = countvectorizer.fit_transform(corpus)\n",
    "    print(countvector.shape)\n",
    "    tfidf = tfidftransformer.fit_transform(countvector)\n",
    "    print(tfidf.shape)\n",
    "\n",
    "    # 保存到文件，方便下次调用\n",
    "    with open(path+'tfidf.pkl','wb') as file:\n",
    "        pickle.dump(tfidf, file)\n",
    "else:\n",
    "    # 调用上次处理的结果\n",
    "    with open(path+'tfidf.pkl','rb') as file:\n",
    "        tfidf = pickle.load(file)\n",
    "\n",
    "#print(type(tfidf))\n",
    "# 将csr_matrix 转换为 numpy.ndarray类型, 同时将原来float64类型转换为float32类型\n",
    "tfidf = tfidf.toarray().astype(np.float32)\n",
    "# embedding的维度\n",
    "d = tfidf.shape[1]\n",
    "print(d)\n",
    "print(tfidf.shape)\n",
    "print(type(tfidf))\n",
    "#print(tfidf[1])\n",
    "print(type(tfidf[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精确索引\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(d)  # 构建 IndexFlatL2\n",
    "print(index.is_trained)  # False时需要train\n",
    "index.add(tfidf)  #添加数据\n",
    "print(index.ntotal)  #index中向量的个数\n",
    "\n",
    "#精确索引无需训练便可直接查询\n",
    "k = 10  # 返回结果个数\n",
    "cpindex = 3352\n",
    "query_self = tfidf[cpindex:cpindex+1]  # 查询本身\n",
    "dis, ind = index.search(query_self, k)\n",
    "print(dis.shape) # 打印张量 (5, 10)\n",
    "print(ind.shape) # 打印张量 (5, 10)\n",
    "print(dis)  # 升序返回每个查询向量的距离\n",
    "print(ind)  # 升序返回每个查询向量\n",
    "\n",
    "print('怀疑抄袭:\\n', news.iloc[cpindex].content)\n",
    "# 找一篇相似的原文\n",
    "similar2 = ind[0][1]\n",
    "print(similar2)\n",
    "print('相似原文:\\n', news.iloc[similar2].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
