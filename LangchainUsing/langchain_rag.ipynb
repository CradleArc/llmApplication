{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RAG需要从向量数据库检索上下文然后输入LLM进行生成，因此需要提前将文本数据向量化并存储到向量数据库。主要步骤如下：\n",
    "准备文本资料\n",
    "将文本分块\n",
    "嵌入以及存储块到向量数据库"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdocument_loaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TextLoader \u001B[38;5;66;03m# 文本加载器\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharacterTextSplitter \u001B[38;5;66;03m# 文本分块器\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OllamaEmbeddings \u001B[38;5;66;03m# Ollama向量嵌入器\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader # 文本加载器\n",
    "from langchain.text_splitter import CharacterTextSplitter # 文本分块器\n",
    "from langchain_community.embeddings import OllamaEmbeddings # Ollama向量嵌入器\n",
    "import weaviate # 向量数据库\n",
    "from weaviate.embedded import EmbeddedOptions # 向量嵌入选项\n",
    "from langchain.prompts import ChatPromptTemplate # 聊天提示模板\n",
    "from langchain_community.chat_models import ChatOllama # ChatOllma聊天模型\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser # 输出解析器\n",
    "from langchain_community.vectorstores import Weaviate # 向量数据库\n",
    "import requests"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:00:24.470426800Z",
     "start_time": "2024-05-29T17:00:23.201843200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 下载&加载语料\n",
    "这里使用拜登总统2022年的国情咨文作为示例。文件链接https://raw.githubusercontent.com/langchain-ai/langchain/mast...。langchain提供了多个文档加载器，这里我们使用TextLoaders即可。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 下载文件\n",
    "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "# 加载文件\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于原始文档过大，超出了LLM的上下文窗口，需要将其分块才能让LLM识别。LangChain 提供了许多内置的文本分块工具，这里用CharacterTextSplitter作为示例："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "嵌入以及存储到向量数据库\n",
    "为了对语料分块进行搜索，需要为每个块生成向量并嵌入文档，最后将文档和向量一起存储。这里使用Ollama&llama3生成向量，并存储到Weaviate向量数据库。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions()\n",
    ")\n",
    "print(\"store vector\")\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client=client,\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3\"),\n",
    "    by_text=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "检索 & 增强\n",
    "向量数据库加载数据后，可以作为检索器，通过用户查询和嵌入向量之间的语义相似性获取数据，然后使用一个固定的聊天模板即可。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 检索器\n",
    "retriever = vectorstore.as_retriever()\n",
    "# LLM提示模板\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "   Use the following pieces of retrieved context to answer the question.\n",
    "   If you don't know the answer, just say that you don't know.\n",
    "   Use three sentences maximum and keep the answer concise.\n",
    "   Question: {question}\n",
    "   Context: {context}\n",
    "   Answer:\n",
    "   \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后，将检索器、聊天模板以及LLM组合成RAG链就可以了。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3\", temperature=10)\n",
    "rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} # 上下文信息\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "# 开始查询&生成\n",
    "query = \"What did the president mainly say?\"\n",
    "print(rag_chain.invoke(query))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到还是像那么回事的，LLM使用的输入预料的内容答复了一些关于新冠疫情以及工作、社区等内容。\n",
    "\n",
    "langchain支持多种LLM，有需要的读者可以尝试下使用OpenAI提供的LLM。\n",
    "读者可以根据需要替换下输入预料，构造自己的私有知识检索库。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import requests\n",
    "# 下载数据\n",
    "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "# 加载数据\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "# 文本分块\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "# 初始化向量数据库并嵌入目标文档\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions()\n",
    ")\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client=client,\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"llama3\"),\n",
    "    by_text=False\n",
    ")\n",
    "# 检索器\n",
    "retriever = vectorstore.as_retriever()\n",
    "# LLM提示模板\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "   Use the following pieces of retrieved context to answer the question.\n",
    "   If you don't know the answer, just say that you don't know.\n",
    "   Use three sentences maximum and keep the answer concise.\n",
    "   Question: {question}\n",
    "   Context: {context}\n",
    "   Answer:\n",
    "   \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOllama(model=\"llama3\", temperature=10)\n",
    "rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "# 开始查询&生成\n",
    "query = \"What did the president mainly say?\"\n",
    "print(rag_chain.invoke(query))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
