{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098e1d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\cheny\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89611, 7)\n",
      "      id      author                  source  \\\n",
      "0  89617         NaN  快科技@http://www.kkj.cn/   \n",
      "1  89616         NaN  快科技@http://www.kkj.cn/   \n",
      "2  89615         NaN  快科技@http://www.kkj.cn/   \n",
      "3  89614         NaN                     新华社   \n",
      "4  89613  胡淑丽_MN7479                   深圳大件事   \n",
      "\n",
      "                                             content  \\\n",
      "0  此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...   \n",
      "1  骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...   \n",
      "2  此前的一加3T搭载的是3400mAh电池，DashCharge快充规格为5V/4A。\\r\\n...   \n",
      "3    这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\r\\n   \n",
      "4  （原标题：44岁女子跑深圳约会网友被拒，暴雨中裸身奔走……）\\r\\n@深圳交警微博称：昨日清...   \n",
      "\n",
      "                                             feature  \\\n",
      "0  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"37\"...   \n",
      "1  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"15\"...   \n",
      "2  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"18\"...   \n",
      "3  {\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...   \n",
      "4  {\"type\":\"新闻\",\"site\":\"网易热门\",\"commentNum\":\"978\",...   \n",
      "\n",
      "                           title  \\\n",
      "0           小米MIUI 9首批机型曝光：共计15款   \n",
      "1     骁龙835在Windows 10上的性能表现有望改善   \n",
      "2      一加手机5细节曝光：3300mAh、充半小时用1天   \n",
      "3  葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）   \n",
      "4       44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随   \n",
      "\n",
      "                                                 url  \n",
      "0     http://www.cnbeta.com/articles/tech/623597.htm  \n",
      "1     http://www.cnbeta.com/articles/tech/623599.htm  \n",
      "2     http://www.cnbeta.com/articles/tech/623601.htm  \n",
      "3  http://world.huanqiu.com/hot/2017-06/10866126....  \n",
      "4  http://news.163.com/17/0618/00/CN617P3Q0001875...  \n",
      "         id author     source content  \\\n",
      "100   89517    NaN  中国证券报?中证网     NaN   \n",
      "103   89514    NaN  中国证券报?中证网     NaN   \n",
      "997   88620    NaN        央广网     NaN   \n",
      "1273  88344    NaN        央广网     NaN   \n",
      "1282  88335    NaN        央广网     NaN   \n",
      "\n",
      "                                                feature  \\\n",
      "100   {\"type\":\"公司\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
      "103   {\"type\":\"公司\",\"site\":\"中证网\",\"commentNum\":\"0\",\"jo...   \n",
      "997   {\"type\":\"时事要闻\",\"site\":\"参考消息\",\"commentNum\":\"0\",...   \n",
      "1273  {\"type\":\"IT业界\",\"site\":\"参考消息\",\"commentNum\":\"0\",...   \n",
      "1282  {\"type\":\"IT业界\",\"site\":\"参考消息\",\"commentNum\":\"0\",...   \n",
      "\n",
      "                                 title  \\\n",
      "100       天和防务股东未来6个月内计划减持不超过480万股公司股份   \n",
      "103                    晶盛机电调整限制性股票回购价格   \n",
      "997              [主播不在家]第二季：主播陈亮体验垃圾清运   \n",
      "1273                LKK洛可可：想象力经济时代或已到来   \n",
      "1282  CES2017：京东发布两款叮咚智能音箱新品 开放Alpha平台   \n",
      "\n",
      "                                                    url  \n",
      "100   http://www.cs.com.cn/ssgs/gsxw/201706/t2017062...  \n",
      "103   http://www.cs.com.cn/ssgs/gsxw/201706/t2017062...  \n",
      "997   http://www.cankaoxiaoxi.com/china/20170623/214...  \n",
      "1273  http://www.cankaoxiaoxi.com/science/20170610/2...  \n",
      "1282  http://www.cankaoxiaoxi.com/science/20170610/2...  \n",
      "(87054, 7)\n",
      "此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作。有人猜测这也是将精力主要用到MIUI 9的研发之中。\r\n",
      "MIUI 8去年5月发布，距今已有一年有余，也是时候更新换代了。\r\n",
      "当然，关于MIUI 9的确切信息，我们还是等待官方消息。\r\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.782 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 当然 MIUI9 确切 信息 等待 官方消息 更新换代  发布 含 开发 版 体验版 内测 稳定版 暂不受 影响 确保 工程师 集中 全部 精力 进行 系统优化 工作 有人 猜测 精力 主要 用到 MIUI9 研发 之中 \n",
      "(87054, 884)\n",
      "(87054, 884)\n",
      "accuracy: 0.889726997740935\n",
      "precison: 0.9623849539815926\n",
      "recall: 0.9141011022424933\n",
      "f1_score: 0.9376218323586745\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# 加载停用词\n",
    "with open('chinese_stopwords.txt','r', encoding='utf-8') as file:\n",
    "    stopwords=[i[:-1] for i in file.readlines()]\n",
    "#stopwords = [line.strip() for line in open('chinsesstoptxt.txt',encoding='UTF-8').readlines()]\n",
    "\n",
    "# 数据加载\n",
    "news = pd.read_csv('sqlResult.csv',encoding='gb18030')\n",
    "print(news.shape)\n",
    "print(news.head(5))\n",
    "\n",
    "\n",
    "# 处理缺失值\n",
    "print(news[news.content.isna()].head(5))\n",
    "news=news.dropna(subset=['content'])\n",
    "print(news.shape)\n",
    "\n",
    "\n",
    "# 分词\n",
    "def split_text(text):\n",
    "    #return ' '.join([w for w in list(jieba.cut(re.sub('\\s|[%s]' % (punctuation),'',text))) if w not in stopwords])\n",
    "    text = text.replace(' ', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text2 = jieba.cut(text.strip())\n",
    "    result = ' '.join([w for w in text2 if w not in stopwords])\n",
    "    return result\n",
    "\n",
    "print(news.iloc[0].content)\n",
    "print(split_text(news.iloc[0].content))\n",
    "\n",
    "if not os.path.exists(\"corpus.pkl\"):\n",
    "    # 对所有文本进行分词\n",
    "    corpus=list(map(split_text,[str(i) for i in news.content]))\n",
    "    print(corpus[0])\n",
    "    print(len(corpus))\n",
    "    print(corpus[1])\n",
    "    # 保存到文件，方便下次调用\n",
    "    with open('corpus.pkl','wb') as file:\n",
    "        pickle.dump(corpus, file)\n",
    "else:\n",
    "    # 调用上次处理的结果\n",
    "    with open('corpus.pkl','rb') as file:\n",
    "        corpus = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# 得到corpus的TF-IDF矩阵\n",
    "countvectorizer = CountVectorizer(encoding='gb18030',min_df=0.015)\n",
    "tfidftransformer = TfidfTransformer()\n",
    "\n",
    "countvector = countvectorizer.fit_transform(corpus)\n",
    "print(countvector.shape)\n",
    "\n",
    "tfidf = tfidftransformer.fit_transform(countvector)\n",
    "print(tfidf.shape)\n",
    "\n",
    "# 标记是否为自己的新闻\n",
    "label=list(map(lambda source: 1 if '新华' in str(source) else 0,news.source))\n",
    "#print(label)\n",
    "\n",
    "# 数据集切分\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf.toarray(), label, test_size = 0.3, random_state=42)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "\"\"\"\n",
    "# 进行CV=3折交叉验证\n",
    "scores=cross_validate(clf, X_train, y_train, scoring=('accuracy','precision','recall','f1'), cv=3, return_train_score=True)\n",
    "pprint(scores)\n",
    "\"\"\"\n",
    "y_predict = clf.predict(X_test)\n",
    "def show_test_reslt(y_true,y_pred):\n",
    "    print('accuracy:',accuracy_score(y_true,y_pred))\n",
    "    print('precison:',precision_score(y_true,y_pred))\n",
    "    print('recall:',recall_score(y_true,y_pred))\n",
    "    print('f1_score:',f1_score(y_true,y_pred))\n",
    "\n",
    "show_test_reslt(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52372724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可能为Copy的新闻条数： 2818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 使用模型检测抄袭新闻\n",
    "prediction = clf.predict(tfidf.toarray())\n",
    "labels = np.array(label)\n",
    "# compare_news_index中有两列：prediction为预测，labels为真实值\n",
    "compare_news_index = pd.DataFrame({'prediction':prediction,'labels':labels})\n",
    "# copy_news_index：可能是Copy的新闻（即找到预测为1，但是实际不是“新华社”）\n",
    "copy_news_index=compare_news_index[(compare_news_index['prediction'] == 1) & (compare_news_index['labels'] == 0)].index\n",
    "# 实际为新华社的新闻\n",
    "xinhuashe_news_index=compare_news_index[(compare_news_index['labels'] == 1)].index\n",
    "print('可能为Copy的新闻条数：', len(copy_news_index))\n",
    "\n",
    "if not os.path.exists(\"label.pkl\"):\n",
    "    # 使用k-means对文章进行聚类\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    from sklearn.cluster import KMeans\n",
    "    normalizer = Normalizer()\n",
    "    scaled_array = normalizer.fit_transform(tfidf.toarray())\n",
    "\n",
    "    # 使用K-Means, 对全量文档进行聚类\n",
    "    kmeans = KMeans(n_clusters=25,random_state=42)\n",
    "    k_labels = kmeans.fit_predict(scaled_array)\n",
    "    # 保存到文件，方便下次调用\n",
    "    with open('label.pkl','wb') as file:\n",
    "        pickle.dump(k_labels, file)\n",
    "    print(k_labels.shape)\n",
    "    print(k_labels[0])\n",
    "else:\n",
    "    # 调用上次处理的结果\n",
    "    with open('label.pkl','rb') as file:\n",
    "        k_labels = pickle.load(file)\n",
    "\n",
    "if not os.path.exists(\"id_class.pkl\"):\n",
    "    # 创建id_class\n",
    "    id_class = {index:class_ for index, class_ in enumerate(k_labels)}\n",
    "    # 保存到文件，方便下次调用\n",
    "    with open('id_class.pkl','wb') as file:\n",
    "        pickle.dump(id_class, file)\n",
    "else:\n",
    "    # 调用上次处理的结果\n",
    "    with open('id_class.pkl','rb') as file:\n",
    "        id_class = pickle.load(file)\n",
    "\n",
    "\n",
    "if not os.path.exists(\"class_id.pkl\"):\n",
    "    from collections import defaultdict\n",
    "    # 创建你class_id字段，key为classId,value为文档index\n",
    "    class_id = defaultdict(set)\n",
    "    for index,class_ in id_class.items():\n",
    "        # 只统计新华社发布的class_id\n",
    "        if index in xinhuashe_news_index.tolist():\n",
    "            class_id[class_].add(index)\n",
    "    # 保存到文件，方便下次调用\n",
    "    with open('class_id.pkl','wb') as file:\n",
    "        pickle.dump(class_id, file)\n",
    "else:\n",
    "    # 调用上次处理的结果\n",
    "    with open('class_id.pkl','rb') as file:\n",
    "        class_id = pickle.load(file)\n",
    "\n",
    "\n",
    "# 输出每个类别的 文档个数\n",
    "count=0\n",
    "for k in class_id:\n",
    "    print(count, len(class_id[k]))\n",
    "    count +=1\n",
    "\n",
    "# 查找相似文本（使用聚类结果进行filter）\n",
    "def find_similar_text(cpindex, top=10):\n",
    "    # 只在新华社发布的文章中查找\n",
    "    dist_dict={i:cosine_similarity(tfidf[cpindex],tfidf[i]) for i in class_id[id_class[cpindex]]}\n",
    "    # 从大到小进行排序\n",
    "    return sorted(dist_dict.items(),key=lambda x:x[1][0], reverse=True)[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "# 指定某篇文章的相似度\n",
    "#print(copy_news_index)\n",
    "cpindex = 3352 # 在copy_news_index\n",
    "#print('是否在新华社', cpindex in xinhuashe_news_index)\n",
    "#print('是否在copy_news', cpindex in copy_news_index)\n",
    "#print('3134是否在新华社', 3134 in xinhuashe_news_index)\n",
    "#print('3134是否在copy_news', 3134 in copy_news_index)\n",
    "\n",
    "#print(cpindex)\n",
    "similar_list = find_similar_text(cpindex)\n",
    "print(similar_list)\n",
    "print('怀疑抄袭:\\n', news.iloc[cpindex].content)\n",
    "# 找一篇相似的原文\n",
    "similar2 = similar_list[0][0]\n",
    "print('相似原文:\\n', news.iloc[similar2].content)\n",
    "# 求任意两篇文章的编辑距离 \n",
    "print('编辑距离:',editdistance.eval(corpus[cpindex], corpus[similar2]))\n",
    "\n",
    "\n",
    "def find_similar_sentence(candidate, raw):\n",
    "    similist = []\n",
    "    cl = candidate.strip().split('。')\n",
    "    ra = raw.strip().split('。')\n",
    "    for c in cl:\n",
    "        for r in ra:\n",
    "            similist.append([c,r,editdistance.eval(c,r)])\n",
    "    # 最相似的5个句子\n",
    "    sort=sorted(similist,key=lambda x:x[2])[:5]\n",
    "    for c,r,ed in sort:\n",
    "        if c!='' and r!='':\n",
    "            print('怀疑抄袭句:{0}\\n相似原句:{1}\\n 编辑距离:{2}\\n'.format(c,r,ed))\n",
    "# 查找copy文章 和第一相似的原文的比对\n",
    "find_similar_sentence(news.iloc[cpindex].content, news.iloc[similar2].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
